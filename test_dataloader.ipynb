{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home2/vanshg/miniconda3/envs/lip-reading/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from data.dataset import AVDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'project_name': 'raven_test', 'experiment_name': None, 'num_workers': 8, 'fix_seed': True, 'slurm_job_id': None, 'train': False, 'log_wandb': True, 'test_on_one_gpu': True, 'data': {'frames_per_gpu_val': 500, 'modality': 'video', 'labels_type': 'unigram1000', 'dataset': {'sample_rate': 16000, 'fps': 25, 'train_csv': 'train_with_tags_counts_unigram1000.csv', 'val_csv': 'val_with_tags_counts_unigram1000.csv', 'test_csv': 'test_with_tags_counts_unigram1000.csv', 'paths': {'root_lrs3_video': None, 'root_lrs3_audio': None, 'root_lrs2_video': None, 'root_lrs2_audio': None}}, 'channel': {'obj': {'_target_': 'torchvision.transforms.Normalize', 'mean': [0.421], 'std': [0.165]}, 'in_video_channels': 1}, 'crop_type': {'random_crop_dim': 88}}, 'logger': {'_target_': 'pytorch_lightning.loggers.WandbLogger', 'log_model': True, 'name': '${experiment_name}', 'offline': False, 'project': '${project_name}', 'mode': 'online', 'entity': None}, 'logging': {'logging_interval': 'step'}, 'model': {'pretrained_model_path': None, 'pretrained_lm_path': None, 'visual_backbone': {'idim': 512, 'adim': 512, 'aheads': 8, 'eunits': 2048, 'elayers': 12, 'transformer_frontend': 'conv3d', 'transformer_input_layer': 'vanilla_linear', 'dropout_rate': 0.1, 'transformer_attn_dropout_rate': 0.1, 'transformer_encoder_attn_layer_type': 'rel_mha', 'macaron_style': False, 'use_cnn_module': False, 'cnn_module_kernel': 31, 'zero_triu': False, 'a_upsample_ratio': 1, 'relu_type': 'swish', 'ddim': '${model.visual_backbone.adim}', 'dheads': '${model.visual_backbone.aheads}', 'dunits': '${model.visual_backbone.eunits}', 'dlayers': 6, 'lsm_weight': 0.1, 'transformer_length_normalized_loss': False, 'rel_pos_type': 'latest', 'layerscale': True, 'init_values': 0.1, 'ff_bn_pre': True, 'post_norm': False, 'gamma_zero': False, 'gamma_init': 0.1, 'mask_init_type': None, 'ctc_type': 'warpctc', 'drop_path': 0.0, 'mtlalpha': 0.1}, 'audio_backbone': {'idim': 512, 'adim': 512, 'aheads': 8, 'eunits': 2048, 'elayers': 12, 'transformer_frontend': 'conv1d', 'transformer_input_layer': 'vanilla_linear', 'dropout_rate': 0.1, 'transformer_attn_dropout_rate': 0.1, 'transformer_encoder_attn_layer_type': 'rel_mha', 'macaron_style': False, 'use_cnn_module': False, 'cnn_module_kernel': 31, 'zero_triu': False, 'a_upsample_ratio': 1, 'relu_type': 'swish', 'ddim': '${model.audio_backbone.adim}', 'dheads': '${model.audio_backbone.aheads}', 'dunits': '${model.audio_backbone.eunits}', 'dlayers': 6, 'lsm_weight': 0.1, 'transformer_length_normalized_loss': False, 'rel_pos_type': 'latest', 'layerscale': True, 'init_values': 0.1, 'ff_bn_pre': True, 'post_norm': False, 'gamma_zero': False, 'gamma_init': 0.1, 'mask_init_type': None, 'ctc_type': 'warpctc', 'drop_path': 0.0, 'mtlalpha': 0.1}, 'language_model': {'pos_enc': 'none', 'embed_unit': 128, 'att_unit': 512, 'head': 8, 'unit': 2048, 'layer': 16, 'dropout_rate': 0.0}}, 'trainer': {'precision': 32, 'num_nodes': 1}, 'decode': {'name': 'default', 'penalty': 0.0, 'ctc_weight': 0.1, 'lm_weight': 0.0, 'beam_size': 40, 'minlenratio': 0.0, 'maxlenratio': 0.0}}\n"
     ]
    }
   ],
   "source": [
    "from hydra import initialize, compose\n",
    "\n",
    "with initialize(version_base=None, config_path=\"conf\"):\n",
    "    cfg = compose(config_name=\"config_test\")\n",
    "\n",
    "print(cfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from data.data_module import DataModule\n",
    "data_module = DataModule(cfg)\n",
    "\n",
    "label_file = '/ssd_scratch/cvit/vanshg/datasets/accented_speakers/eleanor/all_labels.txt'\n",
    "\n",
    "video_transform = data_module._video_transform('val')\n",
    "test_data = AVDataset(label_file, transforms={'video': video_transform})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['▁AND', '▁SO', '▁HE', '▁WOULD', '▁SPEND', '▁MOST', '▁OF', '▁HIS', '▁', 'LU', 'N', 'CH', '▁TIME', 'S', '▁WITH', '▁THIS', '▁ONE', '▁PARTICULAR', '▁BO', 'Y']\n",
      "<class 'dict'>\n",
      "dict_keys(['data', 'label'])\n",
      "torch.Size([129, 88, 88, 1])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home2/vanshg/raven/data/dataset.py:108: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  return {\"data\": data, \"label\": torch.tensor(token_ids)}\n"
     ]
    }
   ],
   "source": [
    "for data in test_data:\n",
    "    print(type(data))\n",
    "    print(data.keys())\n",
    "    print(data['data'].shape)\n",
    "    break"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lip-reading",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
